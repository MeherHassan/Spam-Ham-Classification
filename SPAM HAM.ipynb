{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stopped-ministry",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "realistic-baker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier   \n",
    "from sklearn.model_selection import train_test_split as split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vertical-release",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>word_freq_receive</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_freq_make</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.21</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word_freq_address  word_freq_all  word_freq_3d  word_freq_our  \\\n",
       "word_freq_make                                                                  \n",
       "0.00                         0.64           0.64           0.0           0.32   \n",
       "0.21                         0.28           0.50           0.0           0.14   \n",
       "0.06                         0.00           0.71           0.0           1.23   \n",
       "0.00                         0.00           0.00           0.0           0.63   \n",
       "0.00                         0.00           0.00           0.0           0.63   \n",
       "\n",
       "                word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "word_freq_make                                                         \n",
       "0.00                      0.00              0.00                0.00   \n",
       "0.21                      0.28              0.21                0.07   \n",
       "0.06                      0.19              0.19                0.12   \n",
       "0.00                      0.00              0.31                0.63   \n",
       "0.00                      0.00              0.31                0.63   \n",
       "\n",
       "                word_freq_order  word_freq_mail  word_freq_receive  ...  \\\n",
       "word_freq_make                                                      ...   \n",
       "0.00                       0.00            0.00               0.00  ...   \n",
       "0.21                       0.00            0.94               0.21  ...   \n",
       "0.06                       0.64            0.25               0.38  ...   \n",
       "0.00                       0.31            0.63               0.31  ...   \n",
       "0.00                       0.31            0.63               0.31  ...   \n",
       "\n",
       "                char_freq_;  char_freq_(  char_freq_[  char_freq_!  \\\n",
       "word_freq_make                                                       \n",
       "0.00                   0.00        0.000          0.0        0.778   \n",
       "0.21                   0.00        0.132          0.0        0.372   \n",
       "0.06                   0.01        0.143          0.0        0.276   \n",
       "0.00                   0.00        0.137          0.0        0.137   \n",
       "0.00                   0.00        0.135          0.0        0.135   \n",
       "\n",
       "                char_freq_$  char_freq_#  capital_run_length_average  \\\n",
       "word_freq_make                                                         \n",
       "0.00                  0.000        0.000                       3.756   \n",
       "0.21                  0.180        0.048                       5.114   \n",
       "0.06                  0.184        0.010                       9.821   \n",
       "0.00                  0.000        0.000                       3.537   \n",
       "0.00                  0.000        0.000                       3.537   \n",
       "\n",
       "                capital_run_length_longest  capital_run_length_total  spam  \n",
       "word_freq_make                                                              \n",
       "0.00                                    61                       278     1  \n",
       "0.21                                   101                      1028     1  \n",
       "0.06                                   485                      2259     1  \n",
       "0.00                                    40                       191     1  \n",
       "0.00                                    40                       191     1  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam = pd.read_csv( \"spambase.csv\", index_col = 0)\n",
    "\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "south-graphics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>word_freq_receive</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_freq_make</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.333</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.203</td>\n",
       "      <td>2.430</td>\n",
       "      <td>121</td>\n",
       "      <td>666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.722</td>\n",
       "      <td>20</td>\n",
       "      <td>268</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.729</td>\n",
       "      <td>55</td>\n",
       "      <td>1122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word_freq_address  word_freq_all  word_freq_3d  word_freq_our  \\\n",
       "word_freq_make                                                                  \n",
       "0.0                           0.0           1.02           0.0           0.00   \n",
       "0.0                           0.0           0.23           0.0           0.46   \n",
       "0.0                           0.0           0.36           0.0           0.36   \n",
       "0.0                           0.0           0.00           0.0           0.00   \n",
       "0.0                           0.0           0.56           0.0           0.08   \n",
       "\n",
       "                word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "word_freq_make                                                         \n",
       "0.0                       0.00               0.0                 0.0   \n",
       "0.0                       0.00               0.0                 0.0   \n",
       "0.0                       0.00               0.0                 0.0   \n",
       "0.0                       0.00               0.0                 0.0   \n",
       "0.0                       0.16               0.0                 0.0   \n",
       "\n",
       "                word_freq_order  word_freq_mail  word_freq_receive  ...  \\\n",
       "word_freq_make                                                      ...   \n",
       "0.0                        0.00            0.00                0.0  ...   \n",
       "0.0                        0.23            0.00                0.0  ...   \n",
       "0.0                        0.00            0.00                0.0  ...   \n",
       "0.0                        0.00            0.00                0.0  ...   \n",
       "0.0                        0.00            0.16                0.0  ...   \n",
       "\n",
       "                char_freq_;  char_freq_(  char_freq_[  char_freq_!  \\\n",
       "word_freq_make                                                       \n",
       "0.0                   0.000        0.550        0.000         0.00   \n",
       "0.0                   0.000        0.113        0.000         0.09   \n",
       "0.0                   0.279        0.767        0.139         0.00   \n",
       "0.0                   0.000        0.000        0.000         0.00   \n",
       "0.0                   0.164        0.505        0.000         0.01   \n",
       "\n",
       "                char_freq_$  char_freq_#  capital_run_length_average  \\\n",
       "word_freq_make                                                         \n",
       "0.0                   0.000        0.000                       1.333   \n",
       "0.0                   0.000        0.203                       2.430   \n",
       "0.0                   0.000        0.000                       3.722   \n",
       "0.0                   0.000        0.000                       1.250   \n",
       "0.0                   0.021        0.000                       2.729   \n",
       "\n",
       "                capital_run_length_longest  capital_run_length_total  spam  \n",
       "word_freq_make                                                              \n",
       "0.0                                      5                        28     0  \n",
       "0.0                                    121                       666     0  \n",
       "0.0                                     20                       268     0  \n",
       "0.0                                      2                         5     0  \n",
       "0.0                                     55                      1122     0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_train, spam_test = split(spam, train_size=0.7, random_state=1313) \n",
    "spam_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "proprietary-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = spam_train.drop(['spam'], axis=1)\n",
    "y = spam_train['spam']\n",
    "\n",
    "X_test = spam_test.drop(['spam'], axis=1)\n",
    "y_test = spam_test['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "constitutional-wright",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8680124223602484\n"
     ]
    }
   ],
   "source": [
    "spam_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model using the training sets\n",
    "spam_clf.fit(X, y)\n",
    "\n",
    "print(spam_clf.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "junior-theme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Not-spam  Spam\n",
      "Not-spam      1763   194\n",
      "Spam           231  1032\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89      1957\n",
      "           1       0.84      0.82      0.83      1263\n",
      "\n",
      "    accuracy                           0.87      3220\n",
      "   macro avg       0.86      0.86      0.86      3220\n",
      "weighted avg       0.87      0.87      0.87      3220\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict the response for train dataset\n",
    "train_pred_1 = spam_clf.predict(X)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true=y, y_pred=train_pred_1)\n",
    "\n",
    "print(pd.DataFrame(cm, index=['Not-spam', 'Spam'], \n",
    "                   columns=['Not-spam', 'Spam']))\n",
    "\n",
    "print(classification_report(y_true=y, y_pred=train_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "consolidated-arthur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.929192546583851\n"
     ]
    }
   ],
   "source": [
    "spam_clf = LogisticRegression(solver='liblinear', max_iter=10000)\n",
    "\n",
    "# Fit the model using the training set\n",
    "spam_clf.fit(X, y)\n",
    "\n",
    "print(spam_clf.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "flexible-shannon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9160028964518465\n"
     ]
    }
   ],
   "source": [
    "print(spam_clf.fit(X, y).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-premiere",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-swift",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-crash",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "varied-relations",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "immediate-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "disabled-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"spam.csv\",encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "seventh-gibson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "least-printing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\n",
    "\n",
    "df=df.rename(columns={\"v1\":\"class\",\"v2\":\"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "burning-replacement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['class', 'text'], dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "overhead-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess(text):\n",
    "    #Converting text into lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    #Removing square brackets from the text\n",
    "    text = re.sub('\\[.*?\\]','',text)\n",
    "    \n",
    "    \n",
    "    #Removing links starting with (https or www)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    #Removing <\"text\"> type of text \n",
    "    text = re.sub('<.*?>+','',text)\n",
    "    \n",
    "    #Removing punctuations\n",
    "    text = re.sub(\"[%s]\" % re.escape(string.punctuation),'',text)\n",
    "    \n",
    "    #Removing new lines\n",
    "    text = re.sub(\"\\n\",'',text)\n",
    "    \n",
    "    #Removing alphanumeric numbers \n",
    "    text = re.sub('\\w*\\d\\w*','',text)\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "\n",
    "df['text']=df['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "civil-camping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet=WordNetLemmatizer()\n",
    "\n",
    "def stopwords_remove(text):\n",
    "    text=text.split()\n",
    "    text=[wordnet.lemmatize(word) for word in text if word not in set(stopwords.words('english'))]\n",
    "    text=\" \".join(text)\n",
    "    return(text)\n",
    "\n",
    "\n",
    "df['text']=df['text'].apply(stopwords_remove)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "literary-cuisine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   ham  go jurong point crazy available bugis n great ...\n",
       "1   ham                            ok lar joking wif u oni\n",
       "2  spam  free entry wkly comp win fa cup final tkts may...\n",
       "3   ham                u dun say early hor u c already say\n",
       "4   ham           nah dont think go usf life around though"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "coordinate-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "df['class']=le.fit_transform(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "champion-privilege",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class    0\n",
       "text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "distinguished-macedonia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                               text\n",
       "0      0  go jurong point crazy available bugis n great ...\n",
       "1      0                            ok lar joking wif u oni\n",
       "2      1  free entry wkly comp win fa cup final tkts may...\n",
       "3      0                u dun say early hor u c already say\n",
       "4      0           nah dont think go usf life around though"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fitting-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Embedding , Flatten , Dense ,LSTM , Bidirectional , MaxPooling1D , \n",
    "Conv1D , GlobalMaxPool1D , BatchNormalization,Dropout , GRU)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint , ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fossil-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=list()\n",
    "for i in df['text']:\n",
    "    sentence.append(nltk.word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "opposed-savage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat'], ['ok', 'lar', 'joking', 'wif', 'u', 'oni'], ['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'may', 'text', 'fa', 'receive', 'entry', 'questionstd', 'txt', 'ratetcs', 'apply'], ['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say'], ['nah', 'dont', 'think', 'go', 'usf', 'life', 'around', 'though'], ['freemsg', 'hey', 'darling', 'week', 'word', 'back', 'id', 'like', 'fun', 'still', 'tb', 'ok', 'xxx', 'std', 'chgs', 'send', 'å£', 'rcv'], ['even', 'brother', 'like', 'speak', 'treat', 'like', 'aid', 'patent'], ['per', 'request', 'melle', 'melle', 'oru', 'minnaminunginte', 'nurungu', 'vettam', 'set', 'callertune', 'caller', 'press', 'copy', 'friend', 'callertune'], ['winner', 'valued', 'network', 'customer', 'selected', 'receivea', 'å£', 'prize', 'reward', 'claim', 'call', 'claim', 'code', 'valid', 'hour'], ['mobile', 'month', 'u', 'r', 'entitled', 'update', 'latest', 'colour', 'mobile', 'camera', 'free', 'call', 'mobile', 'update', 'co', 'free']]\n"
     ]
    }
   ],
   "source": [
    "print(sentence[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "appropriate-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2c=Word2Vec(sentence,vector_size=100,window=5,workers=4,min_count=2)\n",
    "words=len(model_w2c.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "engaging-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'glove.6B.100d.txt'\n",
    "model_w2c.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "virtual-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts=df['text']\n",
    "cat=df['class']\n",
    "\n",
    "\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(texts,cat,random_state=42,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "auburn-alexander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Vocab size of texts is *** 6416\n"
     ]
    }
   ],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(xtrain)\n",
    "\n",
    "encoded_docs=tokenizer.texts_to_sequences(xtrain)\n",
    "\n",
    "max_length=max([len(s) for s in sentence])\n",
    "x_train=pad_sequences(encoded_docs,maxlen=max_length,padding='post')\n",
    "y_train=np.array(ytrain)\n",
    "\n",
    "vocab_size= len(tokenizer.word_index)+1\n",
    "print(\"*** Vocab size of texts is ***\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dimensional-appointment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Vocab size of texts is *** 7581\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(xtest)\n",
    "\n",
    "encoded_docs=tokenizer.texts_to_sequences(xtest)\n",
    "\n",
    "#max_length_test=len(max(encoded_docs_test , key=len))\n",
    "\n",
    "x_test=pad_sequences(encoded_docs,maxlen=max_length,padding='post')\n",
    "y_test=np.array(ytest)\n",
    "\n",
    "vocab_size= len(tokenizer.word_index)+1\n",
    "print(\"*** Vocab size of texts is ***\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "impressive-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(filename):\n",
    "    file=open(filename,'r')\n",
    "    \n",
    "    lines=file.readlines()[1:]\n",
    "    #print(lines)\n",
    "    embedding=dict()\n",
    "    for line in lines:\n",
    "        parts=line.split()\n",
    "        \n",
    "        embedding[parts[0]]=np.asarray(parts[1:],dtype='float32')\n",
    "        \n",
    "    return(embedding)\n",
    "        \n",
    "    \n",
    "def get_weight_matrix(vocab,raw_embedding):\n",
    "    vocab_size=len(vocab)+1\n",
    "    \n",
    "    weight_matrix=np.zeros((vocab_size,100))\n",
    "    \n",
    "    for word , i in vocab.items():\n",
    "        vector=raw_embedding.get(word)\n",
    "        if(vector is not None):\n",
    "            #print(vector)\n",
    "            weight_matrix[i]=vector\n",
    "        \n",
    "    return(weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "turkish-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_embedding=load_embedding(\"glove.6B.100d.txt\")    \n",
    "embedding_vectors=get_weight_matrix(tokenizer.word_index,raw_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ruled-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(vocab_size,100,input_length=max_length,trainable=False,weights=[embedding_vectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "legendary-dragon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 80, 100)           758100    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 76, 32)            16032     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 76, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 38, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 1217      \n",
      "=================================================================\n",
      "Total params: 775,349\n",
      "Trainable params: 17,249\n",
      "Non-trainable params: 758,100\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "advance-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "stretch-gateway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4179 samples, validate on 1393 samples\n",
      "Epoch 1/10\n",
      "4064/4179 [============================>.] - ETA: 0s - loss: 0.3942 - accuracy: 0.8504\n",
      "Epoch 00001: val_loss improved from inf to 0.34530, saving model to model.h5\n",
      "4179/4179 [==============================] - 4s 956us/sample - loss: 0.3948 - accuracy: 0.8485 - val_loss: 0.3453 - val_accuracy: 0.8622\n",
      "Epoch 2/10\n",
      "4160/4179 [============================>.] - ETA: 0s - loss: 0.3270 - accuracy: 0.8546\n",
      "Epoch 00002: val_loss improved from 0.34530 to 0.32843, saving model to model.h5\n",
      "4179/4179 [==============================] - 3s 676us/sample - loss: 0.3264 - accuracy: 0.8550 - val_loss: 0.3284 - val_accuracy: 0.8636\n",
      "Epoch 3/10\n",
      "4096/4179 [============================>.] - ETA: 0s - loss: 0.3197 - accuracy: 0.8611 ETA: 0s - los\n",
      "Epoch 00003: val_loss did not improve from 0.32843\n",
      "4179/4179 [==============================] - 2s 546us/sample - loss: 0.3203 - accuracy: 0.8603 - val_loss: 0.3348 - val_accuracy: 0.8607\n",
      "Epoch 4/10\n",
      "4064/4179 [============================>.] - ETA: 0s - loss: 0.3155 - accuracy: 0.8615\n",
      "Epoch 00004: val_loss did not improve from 0.32843\n",
      "4179/4179 [==============================] - 2s 562us/sample - loss: 0.3149 - accuracy: 0.8617 - val_loss: 0.3412 - val_accuracy: 0.8536\n",
      "Epoch 5/10\n",
      "4064/4179 [============================>.] - ETA: 0s - loss: 0.3140 - accuracy: 0.8590\n",
      "Epoch 00005: val_loss did not improve from 0.32843\n",
      "4179/4179 [==============================] - 2s 519us/sample - loss: 0.3129 - accuracy: 0.8593 - val_loss: 0.3340 - val_accuracy: 0.8557\n",
      "Epoch 6/10\n",
      "4096/4179 [============================>.] - ETA: 0s - loss: 0.3050 - accuracy: 0.8623\n",
      "Epoch 00006: val_loss did not improve from 0.32843\n",
      "4179/4179 [==============================] - 2s 586us/sample - loss: 0.3085 - accuracy: 0.8607 - val_loss: 0.3332 - val_accuracy: 0.8471\n",
      "Epoch 7/10\n",
      "4064/4179 [============================>.] - ETA: 0s - loss: 0.3112 - accuracy: 0.8612\n",
      "Epoch 00007: val_loss improved from 0.32843 to 0.32219, saving model to model.h5\n",
      "4179/4179 [==============================] - 2s 554us/sample - loss: 0.3098 - accuracy: 0.8622 - val_loss: 0.3222 - val_accuracy: 0.8571\n",
      "Epoch 8/10\n",
      "4160/4179 [============================>.] - ETA: 0s - loss: 0.3061 - accuracy: 0.8606\n",
      "Epoch 00008: val_loss did not improve from 0.32219\n",
      "4179/4179 [==============================] - 2s 531us/sample - loss: 0.3057 - accuracy: 0.8607 - val_loss: 0.3260 - val_accuracy: 0.8650\n",
      "Epoch 9/10\n",
      "4160/4179 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.8603\n",
      "Epoch 00009: val_loss improved from 0.32219 to 0.31965, saving model to model.h5\n",
      "4179/4179 [==============================] - 2s 532us/sample - loss: 0.3062 - accuracy: 0.8603 - val_loss: 0.3196 - val_accuracy: 0.8629\n",
      "Epoch 10/10\n",
      "4096/4179 [============================>.] - ETA: 0s - loss: 0.3057 - accuracy: 0.8623\n",
      "Epoch 00010: val_loss improved from 0.31965 to 0.30984, saving model to model.h5\n",
      "4179/4179 [==============================] - 2s 539us/sample - loss: 0.3059 - accuracy: 0.8624 - val_loss: 0.3098 - val_accuracy: 0.8636\n"
     ]
    }
   ],
   "source": [
    "optimum=optimizers.Adam(clipvalue=0.5)\n",
    "#model.compile(loss='binary_crossentropy', optimizer=optimum , metrics=['accuracy'])\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"Adam\", metrics=['accuracy'])\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'model.h5', \n",
    "    monitor = 'val_loss', \n",
    "    verbose = 1, \n",
    "    save_best_only = True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor = 'val_loss', \n",
    "    factor = 0.2, \n",
    "    verbose = 1, \n",
    "    patience = 5,                        \n",
    "    min_lr = 0.001\n",
    ")\n",
    "\n",
    "history=model.fit(x_train,y_train,epochs=10,verbose=1,validation_data=(x_test,y_test),batch_size=32,callbacks=[checkpoint,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "turkish-violation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1393/1393 [==============================] - 0s 299us/sample - loss: 0.3098 - accuracy: 0.8636\n",
      "Accuracy of the model on Testing Data is -  86.36037111282349 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "representative-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "raised-locator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.87      1.00      0.93      1202\n",
      "        Spam       0.56      0.03      0.05       191\n",
      "\n",
      "    accuracy                           0.86      1393\n",
      "   macro avg       0.71      0.51      0.49      1393\n",
      "weighted avg       0.82      0.86      0.81      1393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred, target_names = ['Ham','Spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-production",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
